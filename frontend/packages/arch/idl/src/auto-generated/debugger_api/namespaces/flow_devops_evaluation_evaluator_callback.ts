/*
 * Copyright 2025 coze-dev Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
 
// THIS IS AN AUTOGENERATED FILE. DO NOT EDIT THIS FILE DIRECTLY.
/* eslint-disable */
/* tslint:disable */
// @ts-nocheck

import * as flow_devops_evaluation_callback_common from './flow_devops_evaluation_callback_common';

export type Int64 = string | number;

/** https://lilianweng.github.io/posts/2023-06-23-agent/agent-overview.png
 Agents include: Planning, Memory, Tools, etc.
 Here Action refers to the execution of various subcomponents under the Agent */
export enum ActionType {
  Unknown = 0,
  LLMPlanning = 1,
  ToolCall = 2,
  /** Cozing agent thought process */
  CozeVerbose = 100,
}

export enum BuiltinEvaluatorType {
  /** Using the PromptTemplate input by the user as the configuration information of the evaluator, the input and output of the evaluation object are evaluated */
  Prompt = 1,
  /** The user-customized input Python Code is used as the configuration information of the evaluator to evaluate the input and output of the evaluation object */
  PythonCode = 2,
  /** Using the JS Code input by the user as the configuration information of the evaluator, the input and output of the evaluation object are evaluated */
  JSCode = 3,
  /** manual evaluation */
  Manual = 7,
  /** Prompt in development */
  FornaxPrompt = 10,
  /** Coze2.0 referee model evaluator */
  CozePrompt = 11,
  /** The input and output of the evaluation object are evaluated by using the Func static parameters input by the user as the configuration information of the evaluator */
  BuiltinEquals = 10000,
  BuiltinNotEquals = 10001,
  BuiltinContains = 10002,
  BuiltinNotContains = 10003,
  BuiltinIContains = 10004,
  BuiltinNotIContains = 10005,
  BuiltinRegex = 10006,
  BuiltinNotRegex = 10007,
  BuiltinStartsWith = 10008,
  BuiltinNotStartsWith = 10009,
  BuiltinContainsAny = 10010,
  BuiltinNotContainsAny = 10011,
  BuiltinContainsAll = 10012,
  BuiltinNotContainsAll = 10013,
  BuiltinIContainsAny = 10014,
  BuiltinNotIContainsAny = 10015,
  BuiltinIContainsAll = 10016,
  BuiltinNotIContainsAll = 10017,
  BuiltinIsJSON = 10018,
  BuiltinNotIsJSON = 10019,
  /** deprecated */
  BuiltinContainsJSON = 10020,
  /** deprecated */
  BuiltinNotContainsJSON = 10021,
  BuiltinIsValidJSONObject = 10022,
  BuiltinNotIsValidJSONObject = 10023,
  /** single criteria eval (auto check label / unlabeled) */
  BuiltinConcisenessCriteriaEval = 20001,
  BuiltinRelevanceCriteriaEval = 20002,
  BuiltinHarmfulnessCriteriaEval = 20003,
  BuiltinMaliciousnessCriteriaEval = 20004,
  BuiltinHelpfulnessCriteriaEval = 20005,
  BuiltinControversialityCriteriaEval = 20006,
  BuiltinMisogynyCriteriaEval = 20007,
  BuiltinCriminalityCriteriaEval = 20008,
  BuiltinInsensitivityCriteriaEval = 20009,
  BuiltinDepthCriteriaEval = 20010,
  BuiltinCreativityCriteriaEval = 20011,
  BuiltinDetailCriteriaEval = 20012,
  /** must labeled, CotQA */
  BuiltinCorrectnessEval = 20013,
  /** language consistency */
  BuiltinSpecTestLanguageConsistency = 20014,
  /** Reply to Refuse Check */
  BuiltinSpecTestResponseDenialCheck = 20015,
  /** content authenticity */
  BuiltinSpecTestContentAuthenticity = 20016,
  /** content accuracy */
  BuiltinSpecTestContentAccuracy = 20017,
  /** Meet demand */
  BuiltinSpecTestNeedFulfillment = 20018,
  /** timeliness of reply */
  BuiltinSpecTestResponseTimeliness = 20019,
  /** redundancy of recovery */
  BuiltinSpecTestResponseRedundancy = 20020,
  /** fit the character */
  BuiltinSpecTestCharacterConsistency = 20021,
  /** degree of anthropomorphism */
  BuiltinSpecTestAnthropomorphismLevel = 20022,
  /** input-output semantic similarity */
  BuiltinSpecTestIOSematicSimilarity = 20023,
  /** Answer - output semantic similarity */
  BuiltinSpecTestAOSematicSimilarity = 20024,
  /** graph consistency */
  BuiltinSpecTestImageGenerationConsistency = 20025,
  /** Image aesthetics */
  BuiltinSpecTestImageAesthetics = 20026,
  /** reply integrity */
  BuiltinSpecTestResponseCompleteness = 20027,
  /** Text-to-Image Integrity */
  BuiltinSpecTestTextToImageGenerationCompleteness = 20028,
  /** code generation quality */
  BuiltinSpecTestCodeGenerationScoring = 20029,
  /** Calling correctness for coze bots */
  BuiltinSpecTestPluginCallingCorrectness = 20030,
  /** Correctness of imported parameters for cozed bots */
  BuiltinSpecTestPluginParametersCorrectness = 20031,
  /** Workflow call correctness for coze bots */
  BuiltinSpecTestWorkflowCallingCorrectness = 20032,
  /** Workflow imported parameter correctness for cozing bots */
  BuiltinSpecTestWorkflowParametersCorrectness = 20033,
  /** Trigger call correctness for coze bots */
  BuiltinSpecTestTriggerCallingCorrectness = 20034,
  /** Correctness of imported parameters for cozed bots */
  BuiltinSpecTestTriggerParametersCorrectness = 20035,
  /** Coze bots for process orchestration accuracy */
  BuiltinSpecTestChoreographyAccuracy = 20036,
  /** Fornax prompt leak detection */
  BuiltinFornaxPromptLeakDetection = 20200,
  /** custom metrics
System built-in indicators */
  BuiltinDefaultMetric = 30001,
  /** User reporting custom metrics */
  BuiltinCustomMetric = 30002,
}

export enum EvaluatorState {
  Success = 1,
  Fail = 2,
}

/** A full description of an action for an Agent to execute. */
export interface AgentAction {
  agent_type?: ActionType;
  /** The identification, name and other information of the execution tool this time. The format is independently defined by the access party of the evaluation object */
  action_meta?: string;
  /** Additional information to log about the action.
This log can be used in a few ways. First, it can be used to audit
what exactly the LLM predicted to lead to this (Name, Input).
Second, it can be used in future iterations to show the LLMs prior
thoughts. This is useful when (Name, Input) does not contain
full information about the LLM prediction (for example, any `thought`
before the tool/tool_input).
this field is mainly used to show more information for human */
  log?: string;
  /** the input to pass in to the Tool. */
  input?: flow_devops_evaluation_callback_common.Content;
  /** the output of the Agent */
  output?: flow_devops_evaluation_callback_common.Content;
  /** extend information */
  ext?: Record<string, string>;
}

export interface Evaluator {
  type?: Int64;
  /** When registering on the evaluator management platform, RuleMeta information that needs to be passed through is provided. JSON serialization is recommended */
  evaluator_meta?: string;
}

/** Take a one-row dataset as an example:
Column name: input output context person
Column value: "What kind of job is suitable for me" "You are suitable for rest" "I don't like challenges and don't like to contribute" "{Gender: Male, Age: 18, Diploma: Graduated from a famous university}"
Input parameter construction:
  Input: "What kind of job is suitable for me"
  Variables: map {context: "I don't like challenges, I don't like to contribute", person: "{Gender: Male, Age: 18, Diploma: Graduated from a famous university}"}
  Histories: null */
export interface Input {
  /** The input column in the dataset, typically representing the user input in the evaluation Case */
  input?: string;
  /** In the dataset, all columns except the input and output columns are regarded as Variables, with the column name as the key and the column value as the value. */
  variables?: Record<string, flow_devops_evaluation_callback_common.Content>;
  /** In the multi-round evaluation scenario, one row of data in the dataset can be divided into n rounds of evaluation input.
In the nth round of evaluation, Histories passes in the information of [1~ n-1], using Json serialization. The information of the nth round is passed in by the Input field
The information in the first n-1 rounds here is serialized by Json. The serialized schema is formulated by the evaluation task and parsed and used by the evaluator
For example:
Input: "What do I wear when I go out today?"
Histories: [{"human": "I'm in XX District, XX City, what's the weather like today", "assistant": "After checking the weather API, there are thundershowers today, a level 5 gale, and the temperature is about 5 degrees"}] */
  histories?: Array<flow_devops_evaluation_callback_common.Message>;
  /** The output column in the dataset, which generally represents the output expected to be produced by the evaluation object in the evaluation Case, and is usually used as the Reference for the evaluation. It can be a string or a Json serialization. */
  output?: string;
  /** The output information of the evaluation object. The evaluator will use the output in the dataset as a benchmark to evaluate the Prediction of the output of the evaluation object
Prediction can be a string or a JSON structure, which needs to be aligned with the evaluator */
  prediction?: string;
  /** In non-text mode, the evaluator will evaluate the prediction_v2 of the evaluation object output based on output_v2
When in text mode, you can continue to use fields 1 to 5 */
  input_v2?: flow_devops_evaluation_callback_common.Content;
  output_v2?: flow_devops_evaluation_callback_common.Content;
  prediction_v2?: flow_devops_evaluation_callback_common.Content;
  trajectory?: Trajectory;
}

export interface Metrics {
  /** run start time */
  start_time?: Int64;
  /** run end time */
  end_time?: Int64;
}

export interface Result {
  /** rule running status */
  state: EvaluatorState;
  /** Information at the time of error */
  err_msg?: string;
  /** scoring result */
  score?: number;
  /** Information on the scoring process and results */
  reasoning?: string;
  usage?: Usage;
  metrics?: Metrics;
  /** Redundancy, no usage scenarios yet */
  ext?: Record<string, string>;
}

export interface RuleConfig {
  /** ID is used to locate a JSON Schema to parse the following string JSON content */
  id: Int64;
  /** When the user selects an evaluator, they need to provide the configuration content of the evaluator in order to combine it into a working rule.
The configuration content of each evaluator is defined by the evaluator provider. When the evaluator manages the platform to call back the evaluator, it needs to construct the configuration content according to the JSON Schema required by the evaluator */
  data: string;
}

export interface Trajectory {
  /** In end-to-end testing, action information for each step performed within the object is evaluated
The index in the list represents the observed Steps when the evaluation object is executed
Note: As the number of observation nodes increases, the step of an AgentAction will change */
  actions?: Array<AgentAction>;
}

export interface Usage {
  /** Billing information. The consumption of internal total input and output tokens when an evaluation object Playground is executed */
  input_tokens?: Int64;
  output_tokens?: Int64;
}
/* eslint-enable */
