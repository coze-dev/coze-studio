/*
 * Copyright 2025 coze-dev Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
 
// THIS IS AN AUTOGENERATED FILE. DO NOT EDIT THIS FILE DIRECTLY.
/* eslint-disable */
/* tslint:disable */
// @ts-nocheck

import * as flow_devops_evaluation_entity from './flow_devops_evaluation_entity';
import * as flow_devops_evaluation_object_callback from './flow_devops_evaluation_object_callback';
import * as flow_devops_evaluation_callback_common from './flow_devops_evaluation_callback_common';
import * as flow_devops_evaluation_evaluator_callback from './flow_devops_evaluation_evaluator_callback';
import * as flow_devops_evaluation_manual_annotation from './flow_devops_evaluation_manual_annotation';

export type Int64 = string | number;

/** aggregation method */
export enum AggregatorMode {
  Unknown = 0,
  /** Aggregate by review dimension tag */
  EvaluatorTag = 1,
}

/** aggregator type */
export enum AggregatorType {
  Average = 1,
  Sum = 2,
  Max = 3,
  Min = 4,
  /** Use the double type to represent percentages. For example, 50.5% is set to 0.505. */
  PassingRate = 5,
  ExcellentRate = 6,
  /** Manual options are scored, each option is counted and a proportional distribution is given */
  Count = 7,
  Pct50 = 8,
  Pct90 = 9,
  Pct99 = 10,
}

export enum BatchTaskRetryMode {
  All = 1,
  Unsuccessful = 2,
}

export enum BitableStatus {
  Unknown = 0,
  /** Task not started */
  Running = 1,
  /** Mission successful */
  Success = 2,
  /** Mission failed */
  Failed = 3,
}

/** ChainTask template type */
export enum ChainTaskTemplate {
  ChainTaskTemplate_Unknow = 0,
  ChainTaskTemplate_BotTemplate = 1,
}

/** Types of analytical charts */
export enum ChartType {
  Unknown = 0,
  /** indicator card */
  KPI = 1,
  /** Pie chart */
  Pie = 2,
  /** bar chart */
  Bar = 3,
  /** bar stack chart */
  StackedBar = 4,
  /** Radar chart */
  Radar = 5,
  /** line chart */
  Line = 6,
}

export enum DataType {
  /** Default, floating-point value types with decimals */
  Double = 0,
  JSON = 1,
}

/** Evaluator Resource */
export enum EvaluateMode {
  /** manual */
  Manual = 0,
  /** automatic */
  Auto = 1,
  /** reset */
  Reset = 2,
}

/** Correlation Evaluator */
export enum EvaluateScope {
  /** Data row range */
  Row = 0,
  /** All cases take effect by default */
  Case = 1,
}

export enum ExecutionPolicy {
  /** Error stop running */
  StopOnAnyError = 1,
  /** Error continue to execute subsequent tasks */
  ContinueOnAnyError = 2,
  /** Partial failure to continue with subsequent tasks */
  OnlyContinueOnPartialError = 3,
}

export enum ExportCSVSourceType {
  /** Subsequent possible migration orders are reported as server level exports */
  EvaluationTaskReport = 1,
  ContrastReport = 2,
}

export enum FilterField {
  Unknown = 0,
  /** score */
  Score = 1,
  /** numerical value */
  Value = 2,
  /** Evaluation result option value */
  OptionValue = 3,
  /** label */
  Plaintext = 4,
  /** dataset label */
  DataSetTag = 5,
  /** Evaluation dimension, corresponding to ruleID */
  RuleID = 6,
}

export enum FilterLogicOp {
  Unknown = 0,
  And = 1,
  Or = 2,
}

export enum FilterOperatorType {
  Unknown = 0,
  /** equal to */
  Equal = 1,
  /** Not equal to */
  NotEqual = 2,
  /** contain */
  Contains = 3,
  /** Do not include */
  NotContains = 4,
  /** greater than */
  Greater = 5,
  /** greater than or equal to */
  GreaterOrEqual = 6,
  /** less than */
  Less = 7,
  /** less than or equal to */
  LessOrEqual = 8,
  /** empty */
  IsNull = 9,
  /** non-empty */
  IsNotNull = 10,
}

export enum FornaxAgentOpenAPIKey {
  Unknown = 0,
  Invoke = 1,
  Stream = 2,
}

export enum FornaxAgentTmplType {
  Unknown = 0,
  ChatBot = 1,
  OpenAPI = 2,
}

export enum GenAnnotationTaskCustomFilterLanguage {
  Unknown = 0,
  Golang = 1,
  Typescript = 2,
  Python = 3,
}

/** grouping */
export enum GroupMode {
  Unknown = 0,
  /** dataset label */
  DataSetTag = 1,
  /** Group by option */
  OptionResult = 2,
}

export enum ManualStatus {
  /** No manual labeling is required. */
  NoNeed = 0,
  /** Manual labeling is required. */
  Need = 1,
}

export enum ModelResponseFormat {
  Text = 0,
  Markdown = 1,
  JSON = 2,
}

export enum ModelStyle {
  Custom = 0,
  Creative = 1,
  Balance = 2,
  Precise = 3,
}

/** Evaluator Resource */
export enum PromptTemplateFormat {
  PromptTemplateFormat_FString = 0,
  PromptTemplateFormat_Jinja2 = 1,
}

export enum RetryMode {
  All = 1,
  /** Retry unsuccessful rowGroup */
  Unsuccessful = 2,
  /** Specify rowGroupID to retry */
  SpecifyRowGroup = 3,
}

export enum RowGroupRunState {
  Unknown = -1,
  /** in line */
  Queueing = 0,
  /** in progress */
  Processing = 1,
  /** success */
  Success = 2,
  /** fail */
  Fail = 3,
  /** Results to be evaluated */
  Evaluating = 4,
  /** terminate execution */
  Terminal = 5,
}

export enum RowRunState {
  /** Not started */
  Queueing = 0,
  /** successful execution */
  Success = 1,
  /** execution failed */
  Fail = 2,
}

/** Form display element type */
export enum ShowEntityType {
  /** Evaluation object coze bot, corresponding to use 11: CozeEntityMap */
  CozeBot = 1,
  /** Object prompt */
  Prompt = 2,
  /** user information */
  User = 3,
  /** Registered agent */
  Agent = 4,
}

/** SpecialObjectType can be dynamically registered on the evaluation object management platform alone later, and the registered ones will be dynamically allocated */
export enum SpecialObjectType {
  CozeBot = 0,
  Prompt = 1,
  ChainTask = 2,
  /** Applications connected to Fornax, Eino framework is integrated by default */
  FornaxApp = 3,
  /** FornaxAgent provided by CloudIDE */
  FornaxAgent = 4,
  PlaygroundCozeBot = 5,
  PlaygroundCozeBotV2 = 6,
  /** Result Set Batch Evaluation Type */
  EvalResultDatasetCollection = 7,
  PlaygroundCozeModel = 8,
  CiciCrawl = 9,
  /** Custom evaluation rules, idgen id is not enumerable, this enum is only used for search & filter requests */
  Custom = 100,
}

export enum TaskAggrReportGenStatus {
  /** Not generated, task not completed */
  NotGenerated = 0,
  /** Updating */
  Updating = 1,
  /** When running, you can view the report details, complete it, and display the overall score and [Click to view the aggregated report]. */
  Generated = 2,
  /** If the score is updated after completion, it needs to be re-aggregated. */
  NeedUpdate = 3,
}

export enum TaskManualStatus {
  NoNeed = 0,
  Need = 1,
  Completed = 2,
}

export enum TaskMode {
  Unknown = 0,
  /** Platform Manual Operation Case */
  SubmitExec = 1,
  /** Online review scenario */
  OnlineSyncExec = 2,
}

export enum TaskStatus {
  /** The evaluation task is in progress, and the front-end display [running] */
  Processing = 1,
  /** The evaluation task is being created, the initial session rowGroup status and other operations, and the front-end display [Created] */
  Creating = 4,
  /** Rowgroup continuously reviews and adds to tasks, used in online review scenarios */
  Appending = 10,
  /** The execution is completed and manual scoring is required, and the front-end display [manual scoring is required] */
  NeedManualEval = 20,
  /** The user manually cancels the run, and the front-end display [Terminated, the user cancels] */
  Termination = 21,
  /** The system actively terminates the task, and the front-end display [Terminated: System Abnormal] */
  SystemTermination = 22,
  /** The task execution is completed, and the front-end display [Success] */
  Completed = 30,
  /** The execution is complete, all rowGroups fail to execute, and the front-end display [Failed] */
  Error = 31,
  /** Execution completed, partial rowGroup failed. Front-end display [Partial failure] */
  PartialFailure = 41,
  /** The task has been created, waiting for the task to be scheduled for execution */
  AwaitExecution = 42,
  /** Wait for the task to be retried */
  AwaitRetry = 43,
}

export enum UserChangeMode {
  /** Unlimited, users can read, write, and run */
  Default = 0,
  /** Non-editable, non-triggering, readable */
  ReadOnly = 5,
}

export interface BizError {
  err_msg: string;
  err_code: Int64;
}

export interface ChainTask {
  task_id: string;
  task_name?: string;
  /** During the evaluation, the version of chainTask needs to be transmitted from the front end. */
  version?: number;
  model_info?: string;
  chain_task_template?: ChainTaskTemplate;
  prompt_template_format?: PromptTemplateFormat;
  app_id?: string;
}

export interface CiciCrawl {
  bot_id: string;
  name?: string;
  avatar_url?: string;
  crawl_project_id?: string;
}

export interface ColumnRuleInfo {
  rule_id: Int64;
  evaluator_type: Int64;
  /** Custom Evaluator Name */
  evaluator_type_name: string;
  /** rule name */
  name?: string;
  /** Merge cells for dialog group granularity rules */
  granularity?: flow_devops_evaluation_entity.EvaluatorGranularity;
}

export interface CozeAgent {
  AgentID: Int64;
  AgentName: string;
  ModelInfo?: flow_devops_evaluation_entity.ModelInfo;
}

/** Review CozeBot */
export interface CozeBot {
  bot_id?: Int64;
  /** Default 0, is the draft version
deprecated */
  is_draft?: number;
  /** Create case version is empty, the task entity will carry the bot version when the current case runs */
  version?: Int64;
  /** Bot type, default 0, is draft version */
  bot_info_type?: flow_devops_evaluation_object_callback.CozeBotInfoType;
  /** Create case connector_id is empty, the task entity will carry the current case run time bot connector_id */
  connector_id?: string;
  model_info?: flow_devops_evaluation_entity.ModelInfo;
  bot_name?: string;
  avatar_url?: string;
  env?: string;
  bot_version?: string;
  Agents?: Array<CozeAgent>;
}

export interface DashboardRow {
  /** Dataset structure */
  row_id: Int64;
  /** Dataset cells (including input, output, variable FIXME MultiCell after online, discarded */
  cells: Array<string>;
  /** New evaluation report
Large model real output FIXME MultiOutput is discarded after online */
  output: Output;
  /** Evaluator Report */
  row_eval_cell: RowEvalCell;
  /** Level 2 Details rule_group_id
When deprecated is not 0, it means that the row includes the rule_group_id of all general rules and row-level rules. */
  rule_group_id: Int64;
  /** Contains multimodal output, compatible fields, after rolling out 100%, discard Output */
  multi_output?: flow_devops_evaluation_object_callback.Output;
  /** Extend field 2, multimodal display of the dataset */
  multi_cells?: Array<flow_devops_evaluation_callback_common.Content>;
  /** track information */
  trajectory?: flow_devops_evaluation_evaluator_callback.Trajectory;
  run_state?: RowRunState;
  /** deprecated */
  error_message?: string;
  /** Row the associated logID at execution time */
  log_id?: string;
  error?: RowRunError;
  /** In the presence of a row-level evaluator, the group_id of the row-level evaluator */
  row_rule_group_id?: Int64;
  /** Whether to jump evaluation object call trace */
  direct_object_trace?: boolean;
}

export interface DashboardRowGroup {
  row_group_id?: Int64;
  group_name?: string;
  rows: Array<DashboardRow>;
  run_state?: RowGroupRunState;
  tags?: Array<string>;
  /** rowGroup granularity evaluation ruleID - > Results */
  rule_eval_report_map?: Record<Int64, RowEvalReport>;
  serial_num?: number;
}

export interface EvalObject {
  /** Evaluate the type of the object. Each RPC interface is treated as a type */
  object_type: Int64;
  /** When ObjectType = 0, pass this field. When the evaluation object is CozeBot, you need to set the CozeBot information. */
  coze_bot?: CozeBot;
  /** When ObjectType = 1, pass this field. When the evaluation object is EvalPrompt, you need to set the Prompt information */
  prompt?: EvalPrompt;
  /** When ObjectType is other, pass this field */
  object?: flow_devops_evaluation_object_callback.Object;
  /** When ObjectType = 2, pass this field. When the evaluation object is ChainTask, you need to set ChainTask information */
  chain_task?: ChainTask;
  fornax_app_object?: FornaxAppObject;
  fornax_agent_object?: FornaxAgentObject;
  playground_coze_bot_v2?: PlaygroundCozeBotV2;
  /** EvalResultDatasetCollection type object data */
  eval_result_dataset_collection?: EvalResultDatasetCollection;
  playground_coze_model?: PlaygroundCozeModel;
  cici_crawl?: CiciCrawl;
}

export interface EvalPrompt {
  /** A unique identifier for a prompt */
  prompt_id: string;
  /** When evaluating, the prompt version needs to be transmitted from the front end. */
  version?: string;
  name?: string;
}

export interface EvalResultDatasetCollection {
  items: Array<EvalResultDatasetObject>;
}

export interface EvalResultDatasetObject {
  dataset_id: Int64;
  rule_group_id: Int64;
  dataset_name?: string;
}

export interface EvaluateResult {
  /** score */
  score?: number;
  /** Information on the scoring process and results */
  reasoning?: string;
  /** Whether manual scoring is required, when the current rules do not automatically evaluate the results, ManualStatus = ManualStatus */
  manual_status?: ManualStatus;
  /** Evaluator error */
  error?: RowRunError;
  data?: EvaluateResultData;
  /** Row dimension scoring range, such as in the Coze scenario, the user LLM Prompt determines the output scoring range of the evaluator */
  scoring_scope?: flow_devops_evaluation_entity.ScoringScope;
}

export interface EvaluateResultData {
  score?: number;
  value?: string;
  option?: flow_devops_evaluation_entity.EvaluateResultOption;
  plain_text?: string;
  data_type?: flow_devops_evaluation_entity.EvaluateResultDataType;
  value_type?: flow_devops_evaluation_entity.EvaluateResultValueType;
}

export interface FornaxAgentAPI {
  open_api_key?: FornaxAgentOpenAPIKey;
}

export interface FornaxAgentObject {
  agent_id: Int64;
  faas_id: string;
  name?: string;
  avatar_url?: string;
  tmpl_type?: FornaxAgentTmplType;
  /** Target API for evaluation */
  api?: FornaxAgentAPI;
}

export interface FornaxAppObject {
  psm: string;
  env: string;
  cluster: string;
  region: string;
  app_id: string;
  client_id: string;
  /** For secondary search */
  object?: flow_devops_evaluation_object_callback.Object;
}

export interface Output {
  prediction: string;
}

/** Coze2.0Bot */
export interface PlaygroundCozeBotV2 {
  bot_id?: Int64;
  /** Create case version is empty, the task entity will carry the bot version when the current case runs */
  bot_version?: string;
  /** Bot type, default 0, is draft version */
  bot_info_type?: flow_devops_evaluation_object_callback.CozeBotInfoType;
  model_info?: flow_devops_evaluation_entity.ModelInfo;
  bot_name?: string;
  avatar_url?: string;
}

/** Coze2.0 model */
export interface PlaygroundCozeModel {
  /** Model ID */
  model_id?: string;
  /** Temperature, model output randomness, the larger the value, the more random, the smaller the more conservative (0-1] */
  temperature?: number;
  /** Maximum Token Reply */
  max_tokens?: number;
  /** Another model's output randomness, the larger the value, the more random [0, 1] */
  top_p?: number;
  /** When generating, sample the size of the candidate set */
  top_k?: number;
  /** Frequency penalty, adjust the frequency of words in the generated content, the fewer positive words are [-1.0, 1.0] */
  frequency_penalty?: number;
  /** There is a penalty, adjust the frequency of new words in the generated content, avoid repeating words with positive values, and use new words [-1.0, 1.0] */
  presence_penalty?: number;
  /** model reply content format */
  response_format?: ModelResponseFormat;
  /** Model name */
  model_name?: string;
}

export interface RowEvalCell {
  /** Manual scoring, valid only at ManualStatus = Completed */
  manual_result?: EvaluateResult;
  /** Key: rule_id, row-level data, evaluation results for individual rules */
  rule_eval_report_map?: Record<Int64, RowEvalReport>;
  token?: Int64;
  consuming?: Int64;
  create_time?: Int64;
  end_time?: Int64;
  space_id?: Int64;
  /** The total score aggregated by weight of all automated evaluation rules or human scoring dimensions in this row */
  score?: number;
  rule_eval_reports?: Array<RowEvalReport>;
  row_metrics?: Array<RowEvalReport>;
  manual_annotation_reports?: Array<flow_devops_evaluation_manual_annotation.ManualAnnotationLabelTask>;
}

/** The score corresponding to each evaluator */
export interface RowEvalReport {
  /** TODO: Later, you need to convert it to a real evaluatorName, and temporarily use EvaluatorType to represent an evaluator */
  evaluator_type?: Int64;
  row_eval_result?: EvaluateResult;
  /** The name of the evaluator */
  evaluator_type_name?: string;
  weight?: Int64;
  /** If it is a row-level rule, it needs to be displayed in a separate column */
  is_row_evaluator?: boolean;
  name?: string;
}

export interface RowRunError {
  code: Int64;
  message: string;
  /** for prompt platform */
  detail: string;
  BizError?: BizError;
}

export interface ScoringThreshold {
  /** pass rate threshold */
  pass_threshold?: number;
  /** Merit Rate Threshold */
  excellent_threshold?: number;
}

export interface Task {
  id?: Int64;
  status?: TaskStatus;
  dataset_id?: Int64;
  /** The evaluation object entity when running the case, including version information */
  eval_object?: EvalObject;
  /** Number of rows of test data running */
  row_run_cnt?: Int64;
  /** overall task cost */
  token?: Int64;
  /** Overall task time */
  consuming?: Int64;
  /** Task start time */
  start_time?: Int64;
  /** Task execution completion time */
  end_time?: Int64;
  creator_id?: Int64;
  /** log_id to carry out this task */
  log_id?: string;
  /** The reason for the failure of this execution */
  object_output_err?: string;
  /** Task Statistics, RowGroup Dimensions */
  task_stats?: TaskStats;
  /** Task manual scoring statistics, Row dimension */
  task_manual_stats?: TaskManualStats;
  /** Task human scoring status */
  task_manual_status?: TaskManualStatus;
  /** Aggregate report generation status */
  aggr_report_gen_status?: TaskAggrReportGenStatus;
  score?: number;
  passing_rate?: number;
  excellent_rate?: number;
  /** Passing Merit Threshold */
  threshold?: ScoringThreshold;
  /** Deprecated evaluation object overall token consumption */
  object_token_usage?: TokenUsage;
  /** Runtime parameters, JSON serialization */
  runtime_parameter?: string;
  /** Overall token consumption */
  object_token_cost?: TokenUsage;
  /** Dataset name */
  dataset_name?: string;
  /** Original data source ID */
  original_dataset_id?: Int64;
  /** Evaluator overall token consumption */
  evaluator_token_usage?: TokenUsage;
  credit_cost?: number;
  description?: string;
}

export interface TaskManualStats {
  needed_row_count?: Int64;
  completed_row_count?: Int64;
  /** Number of conversation groups requiring group-granular human scoring */
  needed_row_group_count?: Int64;
  /** Number of dialogue groups that have completed group-granular manual scoring */
  completed_row_group_count?: Int64;
}

export interface TaskStats {
  uncompleted_count?: Int64;
  success_count?: Int64;
  fail_count?: Int64;
  /** Number of rows that were successfully executed */
  success_row_count?: Int64;
  /** Number of rows that failed to execute */
  fail_row_count?: Int64;
  /** Total number of rows executed */
  total_row_count?: Int64;
}

export interface TokenUsage {
  /** Input token consumption */
  input_token: Int64;
  /** Output token consumption */
  output_token: Int64;
}
/* eslint-enable */
